# -*- coding: utf-8 -*-
"""Breastcancer_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TiOLua1q-4sbRvh7_CVNeFg5PObiC4JR

# Load and initialize required libraries
"""

! pip install kfp

!pip install google-cloud-pipeline-components

!pip install gcsfs

# Set parameters
project_id = 'ise543-module8-420522'
location = 'us-central1'

from google.cloud import aiplatform
aiplatform.init(project=project_id, location=location)

from kfp.v2.dsl import pipeline
from kfp.v2.dsl import component

"""#Define components

## Load_datset component
"""

import pandas as pd
from sklearn.datasets import load_breast_cancer
data=load_breast_cancer(as_frame=True)
  #write the dataframe to outputpath

df=pd.concat([data.data,data.target],axis=1)
df.to_csv("gs://breast_cancer_543/breast_cancer1.csv",index=False)

from kfp.v2.dsl import OutputPath

@component(packages_to_install=["pandas", "numpy", "fsspec", "gcsfs"])
def load_data(input_dataset_path: str, output_dataset_path: OutputPath('Dataset')):
    import pandas as pd
    import numpy as np

    df = pd.read_csv(input_dataset_path)
    df.to_csv(output_dataset_path, index=False)

"""## Split_dataset component"""

from kfp.v2.dsl import InputPath

@component(packages_to_install=["pandas", "scikit-learn","fsspec", "gcsfs"])
def split_dataset(input_dataset_path:InputPath(),
                  training_dataset: OutputPath('Dataset'),
                  validation_dataset_path: OutputPath('Dataset')):

    import pandas as pd
    from sklearn.model_selection import train_test_split

    df = pd.read_csv(input_dataset_path)
    train_df, val_df = train_test_split(df, test_size=0.20, random_state=91)
    train_df.to_csv(training_dataset, index=False)
    val_df.to_csv(validation_dataset_path, index=False)

"""## Normalizing_training component"""

from kfp.v2.dsl import InputPath,OutputPath

@component(packages_to_install=["sklearn.preprocessing","pandas"])
def Normalize_training(training_dataset_path:InputPath('Dataset'),
                        normalized_dataset:OutputPath('Dataset')):
  import pandas as pd
  df=pd.read_csv(training_dataset_path)

  df_min_max_scaled = df.copy()
  # apply normalization techniques
  for column in df_min_max_scaled.columns:
	  df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())
  df=df_min_max_scaled

  df.to_csv(normalized_dataset,index=False)

"""## Normalized Testing Component"""

from kfp.v2.dsl import InputPath,OutputPath

@component(packages_to_install=["sklearn.preprocessing","pandas"])
def Normalize_testing(validation_dataset_path:InputPath('Dataset'),
                        normalized_dataset:OutputPath('Dataset')):
  import pandas as pd
  df=pd.read_csv(validation_dataset_path)

  df_min_max_scaled = df.copy()
  # apply normalization techniques
  for column in df_min_max_scaled.columns:
	  df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())
  df=df_min_max_scaled

  df.to_csv(normalized_dataset,index=False)

"""## Train_logistic_regression component"""

from kfp.v2.dsl import Dataset,Input,Output,Model,InputPath

@component(packages_to_install=["pandas", "scikit-learn", "joblib"])
def train_logistic_regression(training_dataset_path: InputPath('Dataset'),
                              output_model: Output[Model]):
    import pandas as pd
    from sklearn.linear_model import LogisticRegression
    import joblib

    # Load the training data
    train_df = pd.read_csv(training_dataset_path)

    X_train = train_df.drop('target', axis=1)
    y_train = train_df['target']

    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    # Save the model to the designated output path
    #serialize
    joblib.dump(model, output_model.path)

"""## Evaluate_model component"""

from kfp.v2.dsl import Metrics,InputPath,Input,Output,Model

@component(packages_to_install=["pandas", "scikit-learn", "joblib"])
def evaluate_model(test_dataset_path: InputPath('Dataset'),
                   model: Input[Model],
                   metrics: Output[Metrics]):
    import pandas as pd
    from sklearn.metrics import accuracy_score, f1_score,roc_auc_score
    import joblib

    # Load the test dataset
    test_df = pd.read_csv(test_dataset_path)
    X_test = test_df.drop(columns=['target'])
    y_test = test_df['target']

    # Load the trained model
    trained_model = joblib.load(model.path)

    # Make predictions
    y_pred = trained_model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    auc=roc_auc_score(y_test, y_pred)

    # 'weighted' can be changed based on the need

    # Log the metrics
    metrics.log_metric("accuracy", accuracy)
    metrics.log_metric("f1_score", f1)
    metrics.log_metric("auc_score",auc)

"""# Define pipeline"""

@pipeline(name='data-preparation-and-Normalization-pipeline')
def Breastcancer_pipeline(input_dataset_path:str):
    create_step=load_data(input_dataset_path=input_dataset_path)

    # Pass the output of create_dataset as input for split_dataset
    split_result = split_dataset(input_dataset_path=create_step.output)

    #pass the output of Spliting to normalized_training component
    normalize_training_result=Normalize_training(training_dataset_path=split_result.outputs['training_dataset'])

    #pass the output of spliting to normalized_testing component
    normalize_test_result = Normalize_testing(validation_dataset_path=split_result.outputs['validation_dataset_path'])

    #pass the output of normalize_training to modle training
    trained_model =  train_logistic_regression(training_dataset_path=normalize_training_result.output)

    evaluate_model(
      test_dataset_path=normalize_test_result.output,
      model=trained_model.output
    )

"""#  Compile and run pipeline"""

from kfp.v2 import compiler

compiler.Compiler().compile(
    pipeline_func=Breastcancer_pipeline,
    package_path = 'Breastcancer_pipeline.json'
)

pipeline_job = aiplatform.PipelineJob(
    display_name='Breastcancer_pipeline',
    template_path='Breastcancer_pipeline.json',
    pipeline_root='gs://breast_cancer_543',
    parameter_values={ 'input_dataset_path':'gs://breast_cancer_543/breast_cancer1.csv'},
    enable_caching=True
)

pipeline_job.run()