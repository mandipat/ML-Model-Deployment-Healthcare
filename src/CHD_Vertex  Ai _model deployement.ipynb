{
  "cells": [
    {
      "cell_type": "code",
      "id": "KuLKR8uGmk2HzBpzQWOpZZSd",
      "metadata": {
        "tags": [],
        "id": "KuLKR8uGmk2HzBpzQWOpZZSd"
      },
      "source": [
        "! pip install kfp\n",
        "!pip install google-cloud-pipeline-components\n",
        "!pip install gcsfs\n",
        "!pip install scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set parameters and initialize aiplatform client library"
      ],
      "metadata": {
        "id": "dZ02fQdjreaq"
      },
      "id": "dZ02fQdjreaq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "project_id = 'my-final-project-ise-543'\n",
        "location = 'us-central1'\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "from kfp.v2.dsl import pipeline, component, component"
      ],
      "metadata": {
        "id": "tOdxeOQYrjX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714687468025,
          "user_tz": 420,
          "elapsed": 4710,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "bc567e58-1aff-4f08-ce28-67c600aea50b"
      },
      "id": "tOdxeOQYrjX4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-7d9a5eb213c5>:8: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
            "  from kfp.v2.dsl import pipeline, component, component\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define components"
      ],
      "metadata": {
        "id": "jBFqWQYJsA4Q"
      },
      "id": "jBFqWQYJsA4Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common dataset preparation steps"
      ],
      "metadata": {
        "id": "bO4r6z4jsB2k"
      },
      "id": "bO4r6z4jsB2k"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from kfp.v2.dsl import component, InputPath, OutputPath\n",
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"fsspec\", \"gcsfs\"])\n",
        "def perform_initial_data_preparation(input_dataset_path: str,\n",
        "                                     output_dataset_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = pd.read_csv(input_dataset_path)\n",
        "\n",
        "    # Filling all the Nan value of cigsPerDay with zero for the rows with current Smoker=0\n",
        "    df.loc[df['currentSmoker']==0,['cigsPerDay']]=df.loc[df['currentSmoker']==0,['cigsPerDay']].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "    # create a new label of 0 for all the NA values in education\n",
        "    df['education']=df['education'].fillna(0)\n",
        "\n",
        "\n",
        "    # Clip the column to remove outliers\n",
        "    clipped_column = df['totChol'].clip( upper=600)\n",
        "\n",
        "    # Replace the original column with the clipped column\n",
        "    df['totChol']=clipped_column\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    df.to_csv(output_dataset_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "gksS_BFdsBPA"
      },
      "id": "gksS_BFdsBPA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test train Dataset"
      ],
      "metadata": {
        "id": "AMdOK3yR7Q6U"
      },
      "id": "AMdOK3yR7Q6U"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import component, InputPath, OutputPath\n",
        "\n",
        "\n",
        "@component(packages_to_install=[\"scikit-learn\", \"pandas\"])\n",
        "def split_dataset(input_dataset_path: InputPath('Dataset'),\n",
        "                  train_data_path: OutputPath('Dataset'),\n",
        "                  validation_data_path: OutputPath('Dataset')):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(input_dataset_path)\n",
        "    X = df.drop(columns=['TenYearCHD'])\n",
        "    y = df['TenYearCHD']\n",
        "\n",
        "    train_data, validation_data = train_test_split(df,test_size=0.2,stratify=y)\n",
        "\n",
        "    train_data.to_csv(train_data_path, index=False)\n",
        "\n",
        "    validation_data.to_csv(validation_data_path, index=False)\n"
      ],
      "metadata": {
        "id": "Km1aT3wz7T7Z"
      },
      "id": "Km1aT3wz7T7Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputing values"
      ],
      "metadata": {
        "id": "oUkdAZZw7xs6"
      },
      "id": "oUkdAZZw7xs6"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Output\n",
        "from kfp.v2.dsl import Artifact\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"joblib\",\"scikit-learn\",\"imbalanced-learn==0.11.0\"])\n",
        "def impute_median_training(training_dataset_path: InputPath('Dataset'),\n",
        "                   imputed_dataset_path: OutputPath('Dataset'),\n",
        "                   scaler_path: OutputPath('Artifact'),\n",
        "                   median: OutputPath('Artifact'),\n",
        "                           features: OutputPath('Artifact')):\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.feature_selection import SelectKBest\n",
        "    from sklearn.feature_selection import f_regression\n",
        "\n",
        "\n",
        "    # Load the training dataset\n",
        "    df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    median_values = {}\n",
        "\n",
        "    for column in ['totChol', 'BMI', 'heartRate', 'a1c', 'glucose']:\n",
        "        med = df[column].median()\n",
        "        df[column] = df[column].fillna(med)\n",
        "        median_values[column] = med\n",
        "\n",
        "    median_df = pd.DataFrame(median_values.items(), columns=['Column', 'Median'])\n",
        "\n",
        "    # drop the remaining Na values\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Create a scaler object\n",
        "    target_column = df['TenYearCHD']\n",
        "    features_to_scale = df.drop('TenYearCHD', axis=1)\n",
        "\n",
        "    # Apply StandardScaler to the features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features_array = scaler.fit_transform(features_to_scale)\n",
        "    scaled_features_df = pd.DataFrame(scaled_features_array, columns=features_to_scale.columns)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a scaler object\n",
        "    # Step 1: Initialize SelectKBest with the desired scoring function\n",
        "    selector = SelectKBest(score_func=f_regression, k=10)  # You can adjust k as needed\n",
        "\n",
        "    # Step 2: Fit the selector to your data\n",
        "    X_new = selector.fit_transform(scaled_features_df, target_column)\n",
        "\n",
        "    # Step 3: Get the selected feature indices\n",
        "    selected_features_indices = selector.get_support(indices=True)\n",
        "\n",
        "    # Step 4: Get the names of the selected features\n",
        "    selected_features_names = list(scaled_features_df.columns[selected_features_indices])\n",
        "\n",
        "    # Step 5: Save the selected feature names to an artifact\n",
        "    joblib.dump(selected_features_names, features)\n",
        "\n",
        "    # Save the selected features dataset to the output path\n",
        "    X_selected = scaled_features_df[selected_features_names]\n",
        "\n",
        "\n",
        "    # Combine the scaled features with the target column\n",
        "    result_df = pd.concat([X_selected, target_column], axis=1)\n",
        "\n",
        "\n",
        "    # Save the model to the designated output path\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "\n",
        "      # Save the median dataframe to the output path\n",
        "    median_df.to_csv(median, index=False)\n",
        "\n",
        "    # Save the normalized dataframe to the output path\n",
        "    result_df.to_csv(imputed_dataset_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PTlP0Yce7wt8"
      },
      "id": "PTlP0Yce7wt8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMote training"
      ],
      "metadata": {
        "id": "QvCzbmU7HjV4"
      },
      "id": "QvCzbmU7HjV4"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"joblib\", \"scikit-learn\",\"imbalanced-learn==0.11.0\"])\n",
        "def Smote_training(training_dataset_path: InputPath('Dataset'),\n",
        "                        OS_dataset_path: OutputPath('Dataset')\n",
        "                       ):\n",
        "\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "\n",
        "    # Load the training dataset\n",
        "    df = pd.read_csv(training_dataset_path)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "    y_train = df['TenYearCHD']\n",
        "    X_selected = df.drop(columns=['TenYearCHD'])\n",
        "\n",
        "\n",
        "\n",
        "    from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "    # Oversampling using Random Oversampler\n",
        "    ros = RandomUnderSampler(sampling_strategy=0.5)\n",
        "    X_ros, y_ros = ros.fit_resample(X_selected, y_train)\n",
        "\n",
        "    # Oversampling using SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_smote, y_smote = smote.fit_resample(X_selected, y_train)\n",
        "\n",
        "    oversampled_df = pd.concat([X_smote, y_smote], axis=1)\n",
        "\n",
        "    oversampled_df.to_csv(OS_dataset_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-RNIhLkyE_Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714695584617,
          "user_tz": 420,
          "elapsed": 213,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8224b397-e2c1-46f4-d6d0-f6e8020c6a97"
      },
      "id": "-RNIhLkyE_Yz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imputing Vaidation Dataset"
      ],
      "metadata": {
        "id": "MQa1X6vY-DIk"
      },
      "id": "MQa1X6vY-DIk"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"scipy\", \"joblib\"])\n",
        "def impute_median_validation(\n",
        "        validation_dataset_path: InputPath('Dataset'),\n",
        "        median_path: InputPath('Artifact'),  # medians from training\n",
        "        scaler_path: InputPath('Artifact'),  # scaler from training\n",
        "        imputed_validation_dataset_path: OutputPath('Dataset'),\n",
        "        FS_dataset_path: InputPath('Artifact')):\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "\n",
        "\n",
        "    # Load the validation dataset\n",
        "    df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    # Load the median values from the training dataset\n",
        "    median_df = pd.read_csv(median_path)\n",
        "\n",
        "    # Fill in the missing values with the median values\n",
        "    # Iterate over columns in the test dataset\n",
        "    for column in median_df['Column']:\n",
        "        # Retrieve the median value for the current column\n",
        "        median_value = median_df.loc[median_df['Column'] == column, 'Median'].values[0]\n",
        "        # Fill missing values in the test dataset with the median value\n",
        "        df[column] = df[column].fillna(median_value)\n",
        "\n",
        "    # Drop the remaining missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Load the scaler\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    y_test = df['TenYearCHD']\n",
        "    X_test = df.drop(columns=['TenYearCHD'])\n",
        "\n",
        "\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    X_test_scaled_array=scaler.transform(X_test)\n",
        "\n",
        "    X_test_scaled=pd.DataFrame(X_test_scaled_array, columns=X_test.columns)\n",
        "\n",
        "    df=pd.concat([X_test_scaled, y_test], axis=1)\n",
        "\n",
        "\n",
        "    # Load the list of selected feature names from the training dataset\n",
        "    selected_features_names = joblib.load(FS_dataset_path)\n",
        "\n",
        "    # Select the same features in the test dataset as selected in the training dataset\n",
        "    selected_test_X =df[selected_features_names]\n",
        "\n",
        "    selected_test_df = pd.concat([selected_test_X, y_test], axis=1)\n",
        "\n",
        "    # Drop the remaining missing values\n",
        "    selected_test_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Save the imputed dataframe to the output path\n",
        "    selected_test_df.to_csv(imputed_validation_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "i6-wHOEo-INv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714696169944,
          "user_tz": 420,
          "elapsed": 265,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1edee532-6f92-4593-bf91-5f2ae3cb8286"
      },
      "id": "i6-wHOEo-INv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting features for validation"
      ],
      "metadata": {
        "id": "D62eIMt2IU7z"
      },
      "id": "D62eIMt2IU7z"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"joblib\"])\n",
        "def select_features_test(test_dataset_path: InputPath('Dataset'),\n",
        "                         #FS_dataset_path: InputPath('Artifact'),\n",
        "                         selected_test_features_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "\n",
        "    # Load the test dataset\n",
        "    test_df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "    # Save the selected features in the test dataset to the output path\n",
        "    test_df.to_csv(selected_test_features_path, index=False)\n"
      ],
      "metadata": {
        "id": "WbAlTjR1Hhx3"
      },
      "id": "WbAlTjR1Hhx3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train logistic"
      ],
      "metadata": {
        "id": "f7HKbOiNSIru"
      },
      "id": "f7HKbOiNSIru"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Output\n",
        "from kfp.v2.dsl import Artifact\n",
        "from kfp.v2.dsl import Model\n",
        "from kfp.v2.dsl import Model\n",
        "from kfp.v2.dsl import Input\n",
        "from kfp.v2.dsl import InputPath\n",
        "from kfp.v2.dsl import OutputPath\n",
        "from kfp.v2.dsl import component\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_logistic_regression(training_dataset_path: InputPath('Dataset'),\n",
        "                              trained_model_artifact: Output[Model]):\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import joblib\n",
        "    import os\n",
        "\n",
        "    # Load the training data\n",
        "    train_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "\n",
        "    trained_model = LogisticRegression(max_iter=1000)\n",
        "    trained_model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the model to the designated gcs output path\n",
        "    os.makedirs(trained_model_artifact.path, exist_ok=True)\n",
        "    joblib.dump(trained_model, os.path.join(trained_model_artifact.path, \"model.joblib\"))"
      ],
      "metadata": {
        "id": "6sboFqfvIwb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714696716895,
          "user_tz": 420,
          "elapsed": 218,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "38463bd1-5112-4818-cf51-b7325ca21a08"
      },
      "id": "6sboFqfvIwb4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train model"
      ],
      "metadata": {
        "id": "RBUlTew1SLXK"
      },
      "id": "RBUlTew1SLXK"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_knn(training_dataset_path: InputPath('Dataset'),\n",
        "              trained_model_artifact: Output[Model]):\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    import joblib\n",
        "    import os\n",
        "\n",
        "    # Load the training data\n",
        "    train_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "\n",
        "    trained_model = KNeighborsClassifier()\n",
        "    trained_model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the model to the designated gcs output path\n",
        "    os.makedirs(trained_model_artifact.path, exist_ok=True)\n",
        "    joblib.dump(trained_model, os.path.join(trained_model_artifact.path, \"model.joblib\"))\n",
        "\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_random_forest(training_dataset_path: InputPath('Dataset'),\n",
        "                        trained_model_artifact: Output[Model]):\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    import joblib\n",
        "    import os\n",
        "\n",
        "    # Load the training data\n",
        "    train_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "\n",
        "    trained_model = RandomForestClassifier()\n",
        "    trained_model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the model to the designated gcs output path\n",
        "    os.makedirs(trained_model_artifact.path, exist_ok=True)\n",
        "    joblib.dump(trained_model, os.path.join(trained_model_artifact.path, \"model.joblib\"))\n",
        "\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_naive_bayes(training_dataset_path: InputPath('Dataset'),\n",
        "                      trained_model_artifact: Output[Model]):\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    import joblib\n",
        "    import os\n",
        "\n",
        "    # Load the training data\n",
        "    train_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "\n",
        "    trained_model = GaussianNB()\n",
        "    trained_model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the model to the designated gcs output path\n",
        "    os.makedirs(trained_model_artifact.path, exist_ok=True)\n",
        "    joblib.dump(trained_model, os.path.join(trained_model_artifact.path, \"model.joblib\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "2ItlZsj9JUcw"
      },
      "id": "2ItlZsj9JUcw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@component(packages_to_install=[\"pandas\",\"scikit-learn\", \"joblib\"])\n",
        "def voting_classifier(training_dataset_path: InputPath('Dataset'),\n",
        "                      knn_model: Input[Model],\n",
        "                       rf_model: Input[Model],\n",
        "                       nb_model: Input[Model],\n",
        "                       voting_model_artifact: Output[Model]):\n",
        "\n",
        "    from sklearn.ensemble import VotingClassifier\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "    import os\n",
        "   # Load the training data\n",
        "    train_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "\n",
        "    # Load the trained models\n",
        "\n",
        "    knn_model_loaded = joblib.load(knn_model.path + \"/model.joblib\")\n",
        "    rf_model_loaded = joblib.load(rf_model.path + \"/model.joblib\")\n",
        "    nb_model_loaded = joblib.load(nb_model.path + \"/model.joblib\")\n",
        "\n",
        "    # Create a voting classifier with the loaded models\n",
        "    voting_classifier = VotingClassifier(estimators=[\n",
        "        ('knn', knn_model_loaded),\n",
        "        ('rf', rf_model_loaded),\n",
        "        ('nb', nb_model_loaded)],voting='soft'\n",
        "    )\n",
        "\n",
        "    voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Save the voting classifier to the designated gcs output path\n",
        "    os.makedirs(voting_model_artifact.path, exist_ok=True)\n",
        "    joblib.dump(voting_classifier, os.path.join(voting_model_artifact.path, \"model.joblib\"))\n"
      ],
      "metadata": {
        "id": "XFGkD115X3Hr"
      },
      "id": "XFGkD115X3Hr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate"
      ],
      "metadata": {
        "id": "O7QjAcGESR4U"
      },
      "id": "O7QjAcGESR4U"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Metrics\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def evaluate_model(test_dataset_path: InputPath('Dataset'),\n",
        "                   knn_model: Input[Model],\n",
        "                   rf_model: Input[Model],\n",
        "                   nb_model: Input[Model],\n",
        "                   voting_model: Input[Model],\n",
        "                   lr_model:  Input[Model],\n",
        "                   #svm_model: Input[Model],\n",
        "                  # gb_model:  Input[Model],\n",
        "                   #xgb_model: Input[Model],\n",
        "                   #cat_model: Input[Model],\n",
        "                   best_model_metrics: Output[Metrics]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "    # Load the test dataset\n",
        "    test_df = pd.read_csv(test_dataset_path)\n",
        "    y_test = test_df['TenYearCHD']\n",
        "    X_test = test_df.drop(columns=['TenYearCHD'])\n",
        "\n",
        "\n",
        "    # Load the trained models\n",
        "    knn_model_loaded = joblib.load(knn_model.path + \"/model.joblib\")\n",
        "    rf_model_loaded = joblib.load(rf_model.path + \"/model.joblib\")\n",
        "    nb_model_loaded = joblib.load(nb_model.path + \"/model.joblib\")\n",
        "    voting_model_loaded = joblib.load(voting_model.path + \"/model.joblib\")\n",
        "    lr_model_loaded = joblib.load(lr_model.path + \"/model.joblib\")\n",
        "    #svm_model_loaded = joblib.load(svm_model)\n",
        "    #gb_model_loaded = joblib.load(gb_model)\n",
        "    #\n",
        "\n",
        "    # Make predictions on the test set for each model\n",
        "    knn_pred = knn_model_loaded.predict(X_test)\n",
        "    rf_pred = rf_model_loaded.predict(X_test)\n",
        "    nb_pred = nb_model_loaded.predict(X_test)\n",
        "    voting_pred = voting_model_loaded.predict(X_test)\n",
        "    lr_pred = lr_model_loaded.predict(X_test)\n",
        "    #svm_pred = svm_model_loaded.predict(X_test)\n",
        "    #gb_pred = gb_model_loaded.predict(X_test\n",
        "\n",
        "    # Calculate evaluation metrics for each model\n",
        "    knn_acc = accuracy_score(y_test, knn_pred)\n",
        "    knn_f1 = f1_score(y_test, knn_pred,average='weighted')\n",
        "    rf_acc = accuracy_score(y_test, rf_pred)\n",
        "    rf_f1 = f1_score(y_test, rf_pred,average='weighted')\n",
        "    nb_acc = accuracy_score(y_test, nb_pred)\n",
        "    nb_f1 = f1_score(y_test, nb_pred,average='weighted')\n",
        "    voting_acc = accuracy_score(y_test, voting_pred)\n",
        "    voting_f1 = f1_score(y_test, voting_pred,average='weighted')\n",
        "    lr_f1=f1_score(y_test,lr_pred,average='weighted')\n",
        "\n",
        "    # Determine the best model based on F1 score\n",
        "    best_model = max([('knn', knn_acc, knn_f1),\n",
        "                      ('rf', rf_acc, rf_f1),\n",
        "                      ('nb', nb_acc, nb_f1),\n",
        "                      ('voting', voting_acc, voting_f1)],\n",
        "                     key=lambda x: x[2])\n",
        "\n",
        "    # Log the evaluation metrics of the best model\n",
        "    best_model_metrics.log_metric(\"accuracy\", best_model[1])\n",
        "    best_model_metrics.log_metric(\"f1_score\", best_model[2])\n",
        "    best_model_metrics.log_metric(\"model\", best_model[0])\n",
        "    best_model_metrics.log_metric(\"knn_accuracy\", knn_acc)\n",
        "    best_model_metrics.log_metric(\"knn_f1_score\", knn_f1)\n",
        "    best_model_metrics.log_metric(\"rf_accuracy\", rf_acc)\n",
        "    best_model_metrics.log_metric(\"rf_f1_score\", rf_f1)\n",
        "    #best_model_metrics.log_metric(\"nb_accuracy\", nb_acc)\n",
        "    #best_model_metrics.log_metric(\"nb f1_score\", nb_f1)\n",
        "    best_model_metrics.log_metric(\"voting_accuracy\", voting_acc)\n",
        "    best_model_metrics.log_metric(\"voting_f1_score\", voting_f1)\n",
        "    best_model_metrics.log_metric(\"lr_f1_score\", lr_f1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PEmGyn0NKino"
      },
      "id": "PEmGyn0NKino",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Defintion"
      ],
      "metadata": {
        "id": "vw1lfSKQLpMq"
      },
      "id": "vw1lfSKQLpMq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pipeline\n",
        "from kfp.v2.dsl import pipeline, Output, Dataset\n",
        "\n",
        "@pipeline(name=\"Heart Disease Prediction Pipeline\")\n",
        "def heart_disease_prediction_pipeline(raw_dataset_path: str):\n",
        "\n",
        "\n",
        "    # Perform initial data preparation\n",
        "    preprocess_task = perform_initial_data_preparation(input_dataset_path=raw_dataset_path)\n",
        "\n",
        "    # Split dataset\n",
        "    split_result = split_dataset(input_dataset_path=preprocess_task.output)\n",
        "\n",
        "\n",
        "    # Process training dataset - impute median\n",
        "    training_data_preparation = impute_median_training(training_dataset_path=split_result.outputs['train_data_path'])\n",
        "\n",
        "    # Process validation dataset - impute median\n",
        "    validation_data_preparation = impute_median_validation(validation_dataset_path=split_result.outputs['validation_data_path'],\n",
        "                                                           median_path=training_data_preparation.outputs['median'],\n",
        "                                                           scaler_path=training_data_preparation.outputs['scaler_path'],\n",
        "                                                           FS_dataset_path=training_data_preparation.outputs['features'])\n",
        "\n",
        "        # feture Selection\n",
        "    feature_selection_task = Smote_training(training_dataset_path=training_data_preparation.outputs['imputed_dataset_path'])\n",
        "\n",
        "    #Feature slection validation\n",
        "\n",
        "    feature_selection_validation_task = select_features_test(test_dataset_path=validation_data_preparation.outputs['imputed_validation_dataset_path'])\n",
        "                                                            # FS_dataset_path=feature_selection_task.outputs['features'])\n",
        "\n",
        "      # Train models\n",
        "    train_lr_task = train_logistic_regression(training_dataset_path=feature_selection_task.outputs['OS_dataset_path'])\n",
        "    train_knn_task = train_knn(training_dataset_path=feature_selection_task.outputs['OS_dataset_path'])\n",
        "    train_rf_task = train_random_forest(training_dataset_path=feature_selection_task.outputs['OS_dataset_path'])\n",
        "    train_nb_task = train_naive_bayes(training_dataset_path=feature_selection_task.outputs['OS_dataset_path'])\n",
        "    train_voting_task = voting_classifier(training_dataset_path=feature_selection_task.outputs['OS_dataset_path'],\n",
        "                                          knn_model=train_knn_task.outputs['trained_model_artifact'],\n",
        "                                          rf_model=train_rf_task.outputs['trained_model_artifact'],\n",
        "                                          nb_model=train_nb_task.outputs['trained_model_artifact'])\n",
        "\n",
        "    evaluate_models_task = evaluate_model(\n",
        "      test_dataset_path=feature_selection_validation_task.output,\n",
        "      knn_model=train_knn_task.outputs['trained_model_artifact'],\n",
        "      rf_model=train_rf_task.outputs['trained_model_artifact'],\n",
        "      nb_model=train_nb_task.outputs['trained_model_artifact'],\n",
        "      voting_model=train_voting_task.outputs['voting_model_artifact'],\n",
        "      lr_model=train_lr_task.outputs['trained_model_artifact']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HvHf8alPK1KL"
      },
      "id": "HvHf8alPK1KL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from kfp.v2 import compiler\n",
        "\n",
        "# Compile the pipeline\n",
        "\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=heart_disease_prediction_pipeline,\n",
        "    package_path=\"heart_disease_prediction_pipeline.json\"\n",
        ")\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='heart_disease_prediction',\n",
        "    template_path='heart_disease_prediction_pipeline.json',\n",
        "    pipeline_root='gs://heart_prediction',\n",
        "    parameter_values={\n",
        "      'raw_dataset_path':'gs://heart_prediction/Final Project Dataset (2).csv'\n",
        "    },\n",
        "    enable_caching=True\n",
        ")\n",
        "\n",
        "pipeline_job.run()"
      ],
      "metadata": {
        "id": "R13eY0lmUaZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714697940413,
          "user_tz": 420,
          "elapsed": 59862,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8faa1872-873d-4613-fa34-57742afab269"
      },
      "id": "R13eY0lmUaZN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/757245801734/locations/us-central1/pipelineJobs/heart-disease-prediction-pipeline-20240503005801\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/757245801734/locations/us-central1/pipelineJobs/heart-disease-prediction-pipeline-20240503005801')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/heart-disease-prediction-pipeline-20240503005801?project=757245801734\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/757245801734/locations/us-central1/pipelineJobs/heart-disease-prediction-pipeline-20240503005801 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/757245801734/locations/us-central1/pipelineJobs/heart-disease-prediction-pipeline-20240503005801 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/757245801734/locations/us-central1/pipelineJobs/heart-disease-prediction-pipeline-20240503005801 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/757245801734/locations/us-central1/pipelineJobs/heart-disease-prediction-pipeline-20240503005801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Metrics\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def evaluate_model(test_dataset_path: InputPath('Dataset'),\n",
        "                   model: Input[Model],\n",
        "                   metrics: Output[Metrics]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "    # Load the test dataset\n",
        "    test_df = pd.read_csv(test_dataset_path)\n",
        "    X_test = test_df.drop(columns=['TenYearCHD'])\n",
        "    y_test = test_df['TenYearCHD']\n",
        "\n",
        "    # Load the trained model\n",
        "    model_file_path = model.path + \"/model.joblib\"\n",
        "    trained_model = joblib.load(model_file_path)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = trained_model.predict(X_test)\n",
        "\n",
        "    # Calculate the confusion matrix and extract components\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Log each component of the confusion matrix separately\n",
        "    metrics.log_metric(\"accuracy\", accuracy)\n",
        "    metrics.log_metric(\"f1_score\", f1)"
      ],
      "metadata": {
        "id": "zXiQYCp5JzUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714688336396,
          "user_tz": 420,
          "elapsed": 240,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "36bb6447-2b05-485b-9890-42d6f18f043d"
      },
      "id": "zXiQYCp5JzUH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Final Project "
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
